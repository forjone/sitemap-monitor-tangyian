import os
import json
import requests
import cloudscraper
import yaml
import gzip
import logging
from datetime import datetime, timedelta
from pathlib import Path
from bs4 import BeautifulSoup
import time
import hmac
import hashlib
import base64

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def load_config(config_path='config.yaml'):
    with open(config_path) as f:
        return yaml.safe_load(f)

def process_sitemap(url):
    try:
        scraper = cloudscraper.create_scraper()
        response = scraper.get(url, timeout=10)
        response.raise_for_status()

        content = response.content
        # æ™ºèƒ½æ£€æµ‹gzipæ ¼å¼
        if content[:2] == b'\x1f\x8b':  # gzip magic number
            content = gzip.decompress(content)

        if b'<urlset' in content:
            return parse_xml(content)
        else:
            return parse_txt(content.decode('utf-8'))
    except requests.RequestException as e:
        logging.error(f"Error processing {url}: {str(e)}")
        return []
    except Exception as e:
        logging.error(f"Unexpected error processing {url}: {str(e)}")
        return []

def parse_xml(content):
    urls = []
    soup = BeautifulSoup(content, 'xml')
    for loc in soup.find_all('loc'):
        url = loc.get_text().strip()
        if url:
            urls.append(url)
    return urls

def parse_txt(content):
    return [line.strip() for line in content.splitlines() if line.strip()]

def save_latest(site_name, new_urls):
    base_dir = Path('latest')
    
    # åˆ›å»ºlatestç›®å½•ï¼ˆä¸æ—¥æœŸç›®å½•åŒçº§ï¼‰
    latest_dir = base_dir
    latest_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜latest.json
    latest_file = latest_dir / f'{site_name}.json'
    with open(latest_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(new_urls))

def save_diff(site_name, new_urls):
    base_dir = Path('diff')
        
    # åˆ›å»ºæ—¥æœŸç›®å½•
    today = datetime.now().strftime('%Y%m%d')
    date_dir = base_dir / today
    date_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜å½“æ—¥æ–°å¢æ•°æ®
    file_path = date_dir / f'{site_name}.json'
    mode = 'a' if file_path.exists() else 'w'
    with open(file_path, mode, encoding='utf-8') as f:
        if mode == 'a':
            f.write('\n--------------------------------\n')  # æ·»åŠ åˆ†éš”ç¬¦
        f.write('\n'.join(new_urls) + '\n')  # ç¡®ä¿æ¯ä¸ªURLåéƒ½æœ‰æ¢è¡Œ

def compare_data(site_name, new_urls):
    latest_file = Path('latest') / f'{site_name}.json'
    
    if not latest_file.exists():
        return []
        
    with open(latest_file) as f:
        last_urls = set(f.read().splitlines())
    
    return [url for url in new_urls if url not in last_urls]

def gen_sign(timestamp, secret):
    # æ‹¼æ¥timestampå’Œsecret
    string_to_sign = '{}\n{}'.format(timestamp, secret)
    hmac_code = hmac.new(string_to_sign.encode("utf-8"), digestmod=hashlib.sha256).digest()

    # å¯¹ç»“æœè¿›è¡Œbase64ç¼–ç 
    sign = base64.b64encode(hmac_code).decode('utf-8')
    return sign

def send_feishu_notification(new_urls, config, site_name):
    if not new_urls:
        return
    
    webhook_url = config['feishu']['webhook_url']
    secret = config['feishu'].get('secret')
    
    message = {
        "msg_type": "interactive",
        "card": {
            "header": {
                "title": {"tag": "plain_text", "content": f"ğŸ® {site_name} æ¸¸æˆä¸Šæ–°é€šçŸ¥"},
                "template": "green"
            },
            "elements": [
                {
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": f"**ä»Šæ—¥æ–°å¢ {len(new_urls)} æ¬¾æ¸¸æˆ**\n\n" + "\n".join(f"â€¢ {url}" for url in new_urls[:10])
                    }
                }
            ]
        }
    }

    # å¦‚æœé…ç½®äº† secretï¼Œåˆ™æ·»åŠ ç­¾å
    if secret:
        timestamp = int(time.time())
        sign = gen_sign(timestamp, secret)
        message["timestamp"] = str(timestamp)
        message["sign"] = sign

    logging.info(f"å‘é€é£ä¹¦è¯·æ±‚ payload: {json.dumps(message, ensure_ascii=False)}")
    
    for attempt in range(3):  # é‡è¯•æœºåˆ¶
        try:
            resp = requests.post(webhook_url, json=message)
            logging.info(f"é£ä¹¦å“åº”çŠ¶æ€ç : {resp.status_code}, å“åº”å†…å®¹: {resp.text}")
            resp.raise_for_status()
            
            # æ£€æŸ¥ä¸šåŠ¡çŠ¶æ€ç 
            resp_json = resp.json()
            if resp_json.get("code") != 0:
                logging.error(f"é£ä¹¦APIæŠ¥é”™: {resp_json}")
                continue # è§†ä¸ºå¤±è´¥ï¼Œè§¦å‘é‡è¯•æˆ–è¿›å…¥ä¸‹ä¸€æ¬¡å¾ªç¯

            logging.info("é£ä¹¦é€šçŸ¥å‘é€æˆåŠŸ")
            return
        except requests.RequestException as e:
            logging.error(f"é£ä¹¦é€šçŸ¥å‘é€å¤±è´¥: {str(e)}")
            if attempt < 2:
                logging.info("é‡è¯•å‘é€é€šçŸ¥...")

def main(config_path='config.yaml'):
    config = load_config(config_path)
    
    for site in config['sites']:
        if not site['active']:
            continue
            
        logging.info(f"å¤„ç†ç«™ç‚¹: {site['name']}")
        all_urls = []
        for sitemap_url in site['sitemap_urls']:
            urls = process_sitemap(sitemap_url)
            all_urls.extend(urls)
            
        # å»é‡å¤„ç†
        unique_urls = list({url: None for url in all_urls}.keys())
        new_urls = compare_data(site['name'], unique_urls)
        
        save_latest(site['name'], unique_urls)
        if new_urls:
            save_diff(site['name'], new_urls)
            send_feishu_notification(new_urls, config, site['name'])
            
        # æ¸…ç†æ—§æ•°æ®
        cleanup_old_data(site['name'], config)

def cleanup_old_data(site_name, config):
    data_dir = Path('diff')
    if not data_dir.exists():
        return
        
    # è·å–é…ç½®ä¸­çš„ä¿ç•™å¤©æ•°
    retention_days = config.get('retention_days', 7)
    cutoff = datetime.now() - timedelta(days=retention_days)
    
    # éå†æ‰€æœ‰æ—¥æœŸæ–‡ä»¶å¤¹
    for date_dir in data_dir.glob('*'):
        if not date_dir.is_dir():
            continue
            
        try:
            # è§£ææ–‡ä»¶å¤¹åç§°ä¸ºæ—¥æœŸ
            dir_date = datetime.strptime(date_dir.name, '%Y%m%d')
            if dir_date < cutoff:
                # åˆ é™¤æ•´ä¸ªæ—¥æœŸæ–‡ä»¶å¤¹
                for f in date_dir.glob('*.json'):
                    f.unlink()
                date_dir.rmdir()
                logging.info(f"å·²åˆ é™¤è¿‡æœŸæ–‡ä»¶å¤¹: {date_dir.name}")
        except ValueError:
            # å¿½ç•¥éæ—¥æœŸæ ¼å¼çš„æ–‡ä»¶å¤¹
            continue
        except Exception as e:
            logging.error(f"åˆ é™¤æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {str(e)}")

if __name__ == '__main__':
    main()
